# -*- coding: utf-8 -*-
"""Email_Marketing_Performance_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VGbn8PPBrjFLylyDqSOe55M3HYsf6hXM

# ðŸ“§ Email Marketing Performance Analysis (A/B Test + NLP)

**Goal:** Analyze email marketing campaign performance through A/B testing and natural language processing (NLP).  
**Dataset:** Synthetic dataset (`email_campaign_dataset.csv`, 1500 rows).  
**Environment:** Google Colab or Jupyter Notebook.
"""

# Install dependencies if needed
# !pip install wordcloud scikit-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from statsmodels.stats.proportion import proportions_ztest

import re
from collections import Counter

# Load dataset (ensure it's uploaded in /content if using Colab)
from google.colab import files
uploaded = files.upload()

#do it when content is seen
df = pd.read_csv('/content/email_campaign_dataset.csv')
df['send_date'] = pd.to_datetime(df['send_date'])
print("Rows:", len(df))
df.head()

print("Columns:", df.columns.tolist())
print(f"Overall open rate: {df['opened'].mean():.3f}")
print(f"Overall click rate: {df['clicked'].mean():.3f}")

summary = df.groupby('variant').agg(
    sends=('row_id','count'),
    open_rate=('opened','mean'),
    click_rate=('clicked','mean')
).reset_index()
summary

# A/B test: two-proportion z-test on open rates
counts = df.groupby('variant')['opened'].sum().values
nobs = df.groupby('variant')['opened'].count().values

stat, pval = proportions_ztest(count=counts, nobs=nobs)
print("Z-statistic:", stat, "P-value:", pval)
if pval < 0.05:
    print("âœ… Significant difference between A and B (p < 0.05)")
else:
    print("âŒ No significant difference detected.")

# Time-based analysis
hourly = df.groupby('send_hour')['opened'].mean().reset_index()
plt.figure(figsize=(10,4))
sns.lineplot(data=hourly, x='send_hour', y='opened', marker='o')
plt.title("Open Rate by Send Hour")
plt.xlabel("Hour of Day")
plt.ylabel("Open Rate")
plt.grid(True)
plt.show()

dow = df.groupby('day_of_week')['opened'].mean().reindex(
    ["Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"]
).reset_index()
plt.figure(figsize=(8,4))
sns.barplot(data=dow, x='day_of_week', y='opened')
plt.xticks(rotation=45)
plt.title("Open Rate by Day of Week")
plt.show()

# Segment & device analysis
print("Open rate by list segment:")
print(df.groupby('list_segment')['opened'].mean().sort_values(ascending=False))

print("\nOpen rate by device:")
print(df.groupby('device')['opened'].mean().sort_values(ascending=False))

# NLP on subject lines
def tokenize(text):
    text = str(text).lower()
    text = re.sub(r'[^a-z0-9\s]', '', text)
    return [t for t in text.split() if len(t) > 1]

df['subject_tokens'] = df['subject_line'].apply(tokenize)
all_tokens = [t for tokens in df['subject_tokens'] for t in tokens]
Counter(all_tokens).most_common(20)

# Sentiment correlation with opens
corr = df['subject_sentiment_score'].corr(df['opened'])
print("Correlation between subject sentiment score and opened:", corr)

sent_bin = df.groupby('subject_sentiment_score')['opened'].mean().reset_index()
plt.figure(figsize=(8,4))
sns.barplot(data=sent_bin, x='subject_sentiment_score', y='opened')
plt.title("Open Rate by Subject Sentiment Score")
plt.show()

# Predictive modeling
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

X = df[['variant','send_hour','device','list_segment','word_count_subject','subject_sentiment_score']].copy()
y = df['opened']
X = pd.get_dummies(X, columns=['variant','device','list_segment'], drop_first=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2)

model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:,1]

print(classification_report(y_test, y_pred))
print("ROC AUC:", roc_auc_score(y_test, y_proba))

# Top words by variant
def top_words_for(group_df, n=10):
    tokens = [t for tokens in group_df['subject_tokens'] for t in tokens]
    return Counter(tokens).most_common(n)

print("Top words for variant A (opened):", top_words_for(df[(df['variant']=='A') & (df['opened']==1)]))
print("\nTop words for variant B (opened):", top_words_for(df[(df['variant']=='B') & (df['opened']==1)]))

# Export summary
report = df.groupby(['variant']).agg(
    sends=('row_id','count'),
    open_rate=('opened','mean'),
    click_rate=('clicked','mean')
).reset_index()
report.to_csv('/content/ab_report_summary.csv', index=False)
print("Saved A/B summary to /content/ab_report_summary.csv")